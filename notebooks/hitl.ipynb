{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cc9ea78",
   "metadata": {},
   "source": [
    "# Human-in-the-Loop\n",
    "\n",
    "Our email assistant can be triage emails and use tools to respond to them. But do we actually trust it to manage our inbox? Few would trust an AI to manage their inbox without some human oversight at the start, which is why human-in-the-loop (HITL) is a critical pattern for agent systems.\n",
    "\n",
    "![overview-img](img/overview_hitl.png)\n",
    "\n",
    "## Human-in-the-Loop with LangGraph Interrupts\n",
    "\n",
    "The HITL (Human-In-The-Loop) pattern is useful for applications where decisions require human validation. LangGraph provides built-in support for this through its [interrupt mechanism](https://langchain-ai.github.io/langgraph/concepts/interrupts/), allowing us to pause execution and request human input when needed. Let's add HITL to our email assistant after specific tools are called!\n",
    "\n",
    "### Simple Interrupt Example\n",
    "\n",
    "Let's assume we want a simple agent that can ask the user a question with a tool call and then use that information. The agent needs to stop and wait for the user to provide the information. This is where the `interrupt` function comes in. The `interrupt` function is the core of LangGraph's human-in-the-loop capability:\n",
    "\n",
    "```\n",
    "location = interrupt(ask.question)\n",
    "```\n",
    "\n",
    "When this line executes:\n",
    "1. It raises a `GraphInterrupt` exception, which pauses the graph execution\n",
    "2. It surfaces the value (the question) to the client\n",
    "3. Execution stops at this point until resumed with a `Command`\n",
    "4. When resumed, the function returns the value provided by the human\n",
    "\n",
    "Here's a minimal example of how to implement a basic interrupt with an agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0592f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langgraph.graph import MessagesState, START, END, StateGraph\n",
    "from langchain_core.tools import tool\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.types import Command, interrupt\n",
    "from IPython.display import Image, display\n",
    "\n",
    "@tool\n",
    "def search(query: str):\n",
    "    \"\"\"Call to surf the web.\"\"\"\n",
    "    return f\"I looked up: {query}. Result: It's sunny in San Francisco.\"\n",
    "\n",
    "# We can define a tool definition for `ask_human`\n",
    "class AskHuman(BaseModel):\n",
    "    \"\"\"Ask the human a question\"\"\"\n",
    "    question: str\n",
    "\n",
    "tools = [search, AskHuman]\n",
    "tool_node = ToolNode([search])\n",
    "\n",
    "# Set up the model\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm = init_chat_model(\"openai:gpt-4o\", temperature=0.0)\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "# Define the function that determines whether to continue or not\n",
    "def should_continue(state):\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    # If there is no function call, then we finish\n",
    "    if not last_message.tool_calls:\n",
    "        return END\n",
    "    # If tool call is asking Human, we return that node\n",
    "    elif last_message.tool_calls[0][\"name\"] == \"AskHuman\":\n",
    "        return \"ask_human\"\n",
    "    # Otherwise if there is, we continue\n",
    "    else:\n",
    "        return \"action\"\n",
    "\n",
    "def call_model(state):\n",
    "    messages = state[\"messages\"]\n",
    "    message = llm_with_tools.invoke(messages)\n",
    "    return {\"messages\": [message]}\n",
    "\n",
    "def ask_human(state):\n",
    "    tool_call_id = state[\"messages\"][-1].tool_calls[0][\"id\"]\n",
    "    ask = AskHuman.model_validate(state[\"messages\"][-1].tool_calls[0][\"args\"])\n",
    "    location = interrupt(ask.question)\n",
    "    tool_message = [{\"tool_call_id\": tool_call_id, \"type\": \"tool\", \"content\": location}]\n",
    "    return {\"messages\": tool_message}\n",
    "\n",
    "# Define a new graph\n",
    "workflow = StateGraph(MessagesState)\n",
    "workflow.add_node(\"agent\", call_model)\n",
    "workflow.add_node(\"action\", tool_node)\n",
    "workflow.add_node(\"ask_human\", ask_human)\n",
    "\n",
    "# Set the entrypoint as `agent`\n",
    "workflow.add_edge(START, \"agent\")\n",
    "# We now add a conditional edge\n",
    "workflow.add_conditional_edges(\n",
    "    # First, we define the start node. We use `agent`.\n",
    "    # This means these are the edges taken after the `agent` node is called.\n",
    "    \"agent\",\n",
    "    # Next, we pass in the function that will determine which node is called next.\n",
    "    should_continue,\n",
    ")\n",
    "# We now add a normal edge from `tools` to `agent`.\n",
    "workflow.add_edge(\"action\", \"agent\")\n",
    "workflow.add_edge(\"ask_human\", \"agent\")\n",
    "\n",
    "# Set up memory\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "memory = MemorySaver()\n",
    "app = workflow.compile(checkpointer=memory)\n",
    "display(Image(app.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cdeb06b",
   "metadata": {},
   "source": [
    "Now, we ask the user where they are and look up the weather there:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3469ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "messages = [{\"role\": \"user\", \"content\": \"Ask the user where they are, then look up the weather there\"}]\n",
    "for event in app.stream({\"messages\": messages}, config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d04b7e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "You can see that our graph got interrupted inside the ask_human node. \n",
    "\n",
    "It is now waiting for a location to be provided. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c9ef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "app.get_state(config).next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d734fb2",
   "metadata": {},
   "source": [
    "### Using Command to Resume Execution\n",
    "\n",
    "After an interrupt, we need a way to continue execution. This is where the `Command` interface comes in. The `Command` object has several powerful capabilities:\n",
    "- `resume`: Provides the value to return from the interrupt call\n",
    "- `goto`: Specifies which node to route to next\n",
    "- `update`: Modifies the state before continuing execution\n",
    "- `graph`: Controls navigation between parent and child graphs\n",
    "\n",
    "In this case, the `Command` object serves two crucial purposes:\n",
    "1. It provides the value to be returned from the `interrupt` call\n",
    "2. It controls the flow of execution in the graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147387e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Resume execution with the value \"san francisco\"\n",
    "for event in app.stream(Command(resume=\"san francisco\"), config, stream_mode=\"values\"):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe0bdfb",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a909b3",
   "metadata": {},
   "source": [
    "This makes `Command` a versatile tool for controlling graph flow during human-in-the-loop interactions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba6ac0",
   "metadata": {},
   "source": [
    "## OLDER\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325c8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "from email_assistant.prompts import triage_system_prompt, triage_user_prompt, agent_system_prompt_hitl, default_background, default_triage_instructions, default_response_preferences, default_cal_preferences\n",
    "from email_assistant.schemas import State, RouterSchema, StateInput\n",
    "from email_assistant.utils import parse_email, format_for_display, format_email_markdown"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0d2921",
   "metadata": {},
   "source": [
    "### Tools \n",
    "\n",
    "As before, we define our tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e64a890",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def write_email(to: str, subject: str, content: str) -> str:\n",
    "    \"\"\"Write and send an email.\"\"\"\n",
    "    # Placeholder response - in real app would send email\n",
    "    return f\"Email sent to {to} with subject '{subject}' and content: {content}\"\n",
    "\n",
    "@tool\n",
    "def schedule_meeting(\n",
    "    attendees: list[str], subject: str, duration_minutes: int, preferred_day: str\n",
    ") -> str:\n",
    "    \"\"\"Schedule a calendar meeting.\"\"\"\n",
    "    # Placeholder response - in real app would check calendar and schedule\n",
    "    return f\"Meeting '{subject}' scheduled for {preferred_day} with {len(attendees)} attendees\"\n",
    "\n",
    "@tool\n",
    "def check_calendar_availability(day: str) -> str:\n",
    "    \"\"\"Check calendar availability for a given day.\"\"\"\n",
    "    # Placeholder response - in real app would check actual calendar\n",
    "    return f\"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM\"\n",
    "    \n",
    "@tool\n",
    "class Done(BaseModel):\n",
    "      \"\"\"E-mail has been sent.\"\"\"\n",
    "      done: bool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d04112c",
   "metadata": {},
   "source": [
    "### Interrupt Handler for Triage\n",
    "\n",
    "One key enhancement is adding human oversight at the triage stage. When an email is classified as \"notify,\" we interrupt to get human confirmation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025b6cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triage_interrupt_handler(state: State) -> Command[Literal[\"response_agent\", \"__end__\"]]:\n",
    "    \"\"\"Handles interrupts from the triage step\"\"\"\n",
    "    \n",
    "    # Parse the email input\n",
    "    author, to, subject, email_thread = parse_email(state[\"email_input\"])\n",
    "\n",
    "    # Create email markdown for Agent Inbox in case of notification  \n",
    "    email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "\n",
    "    # Create messages to save to memory\n",
    "    messages = [{\"role\": \"user\",\n",
    "                \"content\": f\"Classification Decision: {state['classification_decision']} for email: {email_markdown}\"\n",
    "                }]\n",
    "\n",
    "    # Create interrupt for Agent Inbox\n",
    "    request = {\n",
    "        \"action_request\": {\n",
    "            \"action\": f\"Email Assistant: {state['classification_decision']}\",\n",
    "            \"args\": {}\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"allow_ignore\": True,  \n",
    "            \"allow_respond\": True, # Allow user feedback if decision is not correct \n",
    "            \"allow_edit\": False, \n",
    "            \"allow_accept\": False,  \n",
    "        },\n",
    "        # Email to show in Agent Inbox\n",
    "        \"description\": email_markdown,\n",
    "    }\n",
    "\n",
    "    # Agent Inbox responds with a list  \n",
    "    response = interrupt([request])[0]\n",
    "\n",
    "    # Accept the decision and end   \n",
    "    if response[\"type\"] == \"accept\":\n",
    "        goto = END \n",
    "\n",
    "    # If user provides feedback, update memory  \n",
    "    elif response[\"type\"] == \"response\":\n",
    "        # Add feedback to messages \n",
    "        user_input = response[\"args\"]\n",
    "        messages.append({\"role\": \"user\",\n",
    "                        \"content\": f\"Here is feedback on how the user would prefer the email to be classified: {user_input}\"\n",
    "                        })\n",
    "\n",
    "        goto = END\n",
    "\n",
    "    # Update the state \n",
    "    update = {\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "    return Command(goto=goto, update=update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61857c91",
   "metadata": {},
   "source": [
    "The interrupt request structure follows Agent Inbox's expected format:\n",
    "1. `action_request`: Defines what action the agent is trying to take\n",
    "2. `config`: Specifies what interaction types are allowed (ignore, respond, edit, accept)\n",
    "3. `description`: Provides context (in this case, the email content) shown in the Agent Inbox UI\n",
    "\n",
    "This structure allows Agent Inbox to render appropriate UI controls and collect user feedback, which we'll be able to test with our local deployment later.\n",
    "\n",
    "### Tool Execution Interrupt Handler\n",
    "\n",
    "The main interrupt handler is responsible for pausing execution before executing certain tools (write_email, schedule_meeting, Question), allowing humans to review, edit, or reject these actions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41f264e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interrupt_handler(state: State):\n",
    "    \"\"\"Creates an interrupt for human review of tool calls\"\"\"\n",
    "    \n",
    "    # Store messages\n",
    "    result = []\n",
    "\n",
    "    # Iterate over the tool calls in the last message\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        \n",
    "        # Define which tools require human oversight\n",
    "        hitl_tools = [\"write_email\", \"schedule_meeting\", \"Question\"]\n",
    "        \n",
    "        # If tool is not in our HITL list, execute it directly without interruption\n",
    "        if tool_call[\"name\"] not in hitl_tools:\n",
    "            # Execute search_memory and other tools without interruption\n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            observation = tool.invoke(tool_call[\"args\"])\n",
    "            result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": tool_call[\"id\"]})\n",
    "            continue\n",
    "            \n",
    "        # Get original email from email_input in state\n",
    "        original_email_markdown = \"\"\n",
    "        if \"email_input\" in state:\n",
    "            email_input = state[\"email_input\"]\n",
    "            author, to, subject, email_thread = parse_email(email_input)\n",
    "            original_email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "        \n",
    "        # Format tool call for display and prepend the original email\n",
    "        tool_display = format_for_display(state, tool_call)\n",
    "        description = original_email_markdown + tool_display\n",
    "\n",
    "        # Configure what actions are allowed in Agent Inbox\n",
    "        if tool_call[\"name\"] == \"write_email\":\n",
    "            config = {\n",
    "                \"allow_ignore\": True,\n",
    "                \"allow_respond\": True,\n",
    "                \"allow_edit\": True,\n",
    "                \"allow_accept\": True,\n",
    "            }\n",
    "        elif tool_call[\"name\"] == \"schedule_meeting\":\n",
    "            config = {\n",
    "                \"allow_ignore\": True,\n",
    "                \"allow_respond\": True,\n",
    "                \"allow_edit\": True,\n",
    "                \"allow_accept\": True,\n",
    "            }\n",
    "        elif tool_call[\"name\"] == \"Question\":\n",
    "            config = {\n",
    "                \"allow_ignore\": True,\n",
    "                \"allow_respond\": True,\n",
    "                \"allow_edit\": False,\n",
    "                \"allow_accept\": False,\n",
    "            }\n",
    "\n",
    "        # Create the interrupt request\n",
    "        request = {\n",
    "            \"action_request\": {\n",
    "                \"action\": tool_call[\"name\"],\n",
    "                \"args\": tool_call[\"args\"]\n",
    "            },\n",
    "            \"config\": config,\n",
    "            \"description\": description,\n",
    "        }\n",
    "\n",
    "        # Send to Agent Inbox and wait for response\n",
    "        response = interrupt([request])[0]\n",
    "\n",
    "        # Handle the responses \n",
    "        if response[\"type\"] == \"accept\":\n",
    "            # Execute the tool with original args\n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            observation = tool.invoke(tool_call[\"args\"])\n",
    "            result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": tool_call[\"id\"]})\n",
    "                        \n",
    "        elif response[\"type\"] == \"edit\":\n",
    "            # Tool selection \n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            \n",
    "            # Get edited args from Agent Inbox\n",
    "            edited_args = response[\"args\"][\"args\"]\n",
    "\n",
    "            # Update the tool calls with edited content\n",
    "            ai_message = state[\"messages\"][-1]\n",
    "            current_id = tool_call[\"id\"]\n",
    "            \n",
    "            # Replace the original tool call with the edited one\n",
    "            ai_message.tool_calls = [tc for tc in ai_message.tool_calls if tc[\"id\"] != current_id] + [\n",
    "                {\"type\": \"tool_call\", \"name\": tool_call[\"name\"], \"args\": edited_args, \"id\": current_id}\n",
    "            ]\n",
    "            \n",
    "            # Execute the tool with edited args\n",
    "            observation = tool.invoke(edited_args)\n",
    "            \n",
    "            # Add only the tool response message\n",
    "            result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": current_id})\n",
    "\n",
    "        elif response[\"type\"] == \"ignore\":\n",
    "            # Don't execute the tool\n",
    "            result.append({\"role\": \"tool\", \"content\": \"Tool execution cancelled by user\", \"tool_call_id\": tool_call[\"id\"]})\n",
    "            \n",
    "        elif response[\"type\"] == \"response\":\n",
    "            # User provided feedback\n",
    "            user_feedback = response[\"args\"]\n",
    "            result.append({\"role\": \"tool\", \"content\": f\"Feedback: {user_feedback}\", \"tool_call_id\": tool_call[\"id\"]})\n",
    "            \n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293e5afe",
   "metadata": {},
   "source": [
    "### Complete Workflow with HITL\n",
    "\n",
    "The complete workflow combines the triage router with interrupt handling and the response agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360fbb07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build response agent\n",
    "agent_builder = StateGraph(State)\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"interrupt_handler\", interrupt_handler)\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"interrupt_handler\": \"interrupt_handler\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"interrupt_handler\", \"llm_call\")\n",
    "response_agent = agent_builder.compile()\n",
    "\n",
    "# Build overall workflow\n",
    "overall_workflow = (\n",
    "    StateGraph(State, input=StateInput)\n",
    "    .add_node(triage_router)\n",
    "    .add_node(triage_interrupt_handler)\n",
    "    .add_node(\"response_agent\", response_agent)\n",
    "    .add_edge(START, \"triage_router\")\n",
    ")\n",
    "\n",
    "email_assistant = overall_workflow.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "943ec68a",
   "metadata": {},
   "source": [
    "## Benefits of the HITL Approach\n",
    "\n",
    "1. **Human Oversight**: Critical actions require human approval before execution\n",
    "2. **Learning Opportunities**: Human feedback is captured and can be used for further training\n",
    "3. **Error Prevention**: Humans can catch and correct mistakes before they happen\n",
    "4. **Trust Building**: Users gain confidence in the system knowing they have final say on important actions\n",
    "5. **Progressive Automation**: As the system proves reliable, oversight can be gradually reduced\n",
    "\n",
    "This HITL implementation showcases how LangGraph's interrupt mechanism combined with Agent Inbox creates a powerful collaboration between human intelligence and AI capabilities, leading to more reliable and trustworthy agent systems.\n",
    "\n",
    "## Testing with Local Deployment\n",
    "\n",
    "LangGraph's interrupt mechanism works in conjunction with [Agent Inbox](https://github.com/langchain-ai/agent-inbox), a user interface designed specifically for human-in-the-loop interactions. When our email assistant needs human input, it creates an interrupt request that appears in Agent Inbox, allowing humans to review, edit, or provide feedback on the agent's actions.\n",
    "\n",
    "![hitl-img](img/hitl.png)\n",
    "\n",
    "As outlined in the README, we'll be able to test our HITL implementation by:\n",
    "\n",
    "1. Running `langgraph dev` to start the LangGraph server locally\n",
    "2. Connecting to Agent Inbox at https://dev.agentinbox.ai/\n",
    "3. Adding a new inbox pointing to our local LangGraph server\n",
    "4. Sending email inputs through LangGraph Studio to trigger interrupts\n",
    "5. Responding to those interrupts in Agent Inbox\n",
    "\n",
    "This setup will allow us to experience the full human-in-the-loop workflow, seeing how our interrupt requests render in Agent Inbox and how we can provide feedback that influences the assistant's behavior."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
