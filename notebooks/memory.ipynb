{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0efe3f3b",
   "metadata": {},
   "source": [
    "# Memory\n",
    "\n",
    "We've used Human-in-the-Loop (HITL) to allow users to review, provide feedback on, or even correct the assistant's decisions. This is great, but it would be even better if the assistant *could learn from* the user's feedback and adapt to their preferences over time. This is where memory comes in. Memory is a critical and emerging component of agent systems, allowing them to learn and improve over time. In this section, we'll add a memory component to our email assistant, allowing it to learn from user feedback and adapt to their preferences over time. This gives us more confidence that the assistant acts on our behalf with personalization. \n",
    "\n",
    "![overview-img](img/overview_memory.png)\n",
    "\n",
    "## Memory in LangGraph\n",
    "\n",
    "### Thread-Scoped and Across-Thread Memory\n",
    "\n",
    "First, it's worth explaining how [memory works in LangGraph](https://langchain-ai.github.io/langgraph/concepts/memory/). LangGraph offers two distinct types of memory that serve complementary purposes in agent systems:\n",
    "\n",
    "**Thread-Scoped Memory (Short-term)** operates within the boundaries of a single conversation thread. It's automatically managed as part of the graph's state and persisted through thread-scoped checkpoints. This memory type retains conversation history, uploaded files, retrieved documents, and other artifacts generated during the interaction. Think of it as the working memory that maintains context within one specific conversation, allowing the agent to reference earlier messages or actions without starting from scratch each time.\n",
    "\n",
    "**Across-Thread Memory (Long-term)** extends beyond individual conversations, creating a persistent knowledge base that spans multiple sessions. This memory is stored as JSON documents in a memory store, organized by namespaces (like folders) and distinct keys (like filenames). Unlike thread-scoped memory, this information persists even after conversations end, enabling the system to recall user preferences, past decisions, and accumulated knowledge. This is what allows an agent to truly learn and adapt over time, rather than treating each interaction as isolated.\n",
    "\n",
    "The [Store](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore) is the foundation of this architecture, providing a flexible database where memories can be organized, retrieved, and updated. What makes this approach powerful is that regardless of which memory type you're working with, the same Store interface provides consistent access patterns. This allows your agent's code to remain unchanged whether you're using a simple in-memory implementation during development or a production-grade database in deployment. \n",
    "\n",
    "### LangGraph Store\n",
    "\n",
    "LangGraph offers different [Store implementations depending on your deployment scenario](https://langchain-ai.github.io/langgraph/reference/store/#langgraph.store.base.BaseStore):\n",
    "\n",
    "1. **Pure In-Memory (e.g., notebooks)**:\n",
    "   - Uses `from langgraph.store.memory import InMemoryStore`\n",
    "   - Purely a Python dictionary in memory with no persistence\n",
    "   - Data is lost when the process terminates\n",
    "   - Useful for quick experiments and testing\n",
    "   - Includes semantic search with cosine similarity\n",
    "\n",
    "2. **Local Development with `langgraph dev`**:\n",
    "   - Similar to InMemoryStore but with pseudo-persistence\n",
    "   - Data is pickled to the local filesystem between restarts\n",
    "   - Lightweight and fast, no need for external databases\n",
    "   - Semantic search uses cosine similarity for embedding comparisons\n",
    "   - Great for development but not designed for production use\n",
    "\n",
    "3. **LangGraph Platform or Production Deployments**:\n",
    "   - Uses PostgreSQL with pgvector for production-grade persistence\n",
    "   - Fully persistent data storage with reliable backups\n",
    "   - Scalable for larger datasets\n",
    "   - High-performance semantic search via pgvector\n",
    "   - Default distance metric is cosine similarity (customizable)\n",
    "\n",
    "Let's use the `InMemoryStore` here in the notebook! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e940a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.memory import InMemoryStore\n",
    "in_memory_store = InMemoryStore()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84702fe9",
   "metadata": {},
   "source": [
    "Memories are namespaced by a tuple, which in this specific example will be (`<user_id>`, \"memories\"). The namespace can be any length and represent anything, does not have to be user specific."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c40c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id = \"1\"\n",
    "namespace_for_memory = (user_id, \"memories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077e7aa",
   "metadata": {},
   "source": [
    "We use the `store.put` method to save memories to our namespace in the store. When we do this, we specify the namespace, as defined above, and a key-value pair for the memory: the key is simply a unique identifier for the memory (memory_id) and the value (a dictionary) is the memory itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1ed147",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_id = str(uuid.uuid4())\n",
    "memory = {\"food_preference\" : \"I like pizza\"}\n",
    "in_memory_store.put(namespace_for_memory, memory_id, memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df1487",
   "metadata": {},
   "source": [
    "We can read out memories in our namespace using the `store.search` method, which will return all memories for a given user as a list. The most recent memory is the last in the list. Each memory type is a Python class (`Item`) with certain attributes. We can access it as a dictionary by converting via `.dict` as above. The attributes it has are shown below, but the most important ones is typically `value`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4737fbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "memories = in_memory_store.search(namespace_for_memory)\n",
    "memories[-1].dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328148f6",
   "metadata": {},
   "source": [
    "To use this in a graph, all we need to do is compile the graph with the store:\n",
    "\n",
    "```\n",
    "# We need this because we want to enable threads (conversations)\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "checkpointer = InMemorySaver()\n",
    "# We need this because we want to enable across-thread memory\n",
    "from langgraph.store.memory import InMemoryStore\n",
    "in_memory_store = InMemoryStore()\n",
    "# Compile the graph with the checkpointer and store\n",
    "graph = graph.compile(checkpointer=checkpointer, store=in_memory_store)\n",
    "```\n",
    "\n",
    "## Memory in LangGraph\n",
    "\n",
    "Let's take our graph used with HITL and add memory to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb4a474",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03da78a5",
   "metadata": {},
   "source": [
    "Here we set up the triage router node, which is the first node in our graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104b7448",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from pydantic import BaseModel\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.types import interrupt, Command\n",
    "\n",
    "from email_assistant.prompts import triage_system_prompt, triage_user_prompt, agent_system_prompt_hitl_memory, default_background, default_triage_instructions, default_response_preferences, default_cal_preferences\n",
    "from email_assistant.schemas import State, RouterSchema, StateInput\n",
    "from email_assistant.utils import parse_email, format_for_display, format_email_markdown\n",
    "\n",
    "# Agent tools \n",
    "@tool\n",
    "def write_email(to: str, subject: str, content: str) -> str:\n",
    "    \"\"\"Write and send an email.\"\"\"\n",
    "    # Placeholder response - in real app would send email\n",
    "    return f\"Email sent to {to} with subject '{subject}' and content: {content}\"\n",
    "\n",
    "@tool\n",
    "def schedule_meeting(\n",
    "    attendees: list[str], subject: str, duration_minutes: int, preferred_day: str\n",
    ") -> str:\n",
    "    \"\"\"Schedule a calendar meeting.\"\"\"\n",
    "    # Placeholder response - in real app would check calendar and schedule\n",
    "    return f\"Meeting '{subject}' scheduled for {preferred_day} with {len(attendees)} attendees\"\n",
    "\n",
    "@tool\n",
    "def check_calendar_availability(day: str) -> str:\n",
    "    \"\"\"Check calendar availability for a given day.\"\"\"\n",
    "    # Placeholder response - in real app would check actual calendar\n",
    "    return f\"Available times on {day}: 9:00 AM, 2:00 PM, 4:00 PM\"\n",
    "\n",
    "@tool\n",
    "class Question(BaseModel):\n",
    "      \"\"\"Question to ask user.\"\"\"\n",
    "      content: str\n",
    "    \n",
    "@tool\n",
    "class Done(BaseModel):\n",
    "      \"\"\"E-mail has been sent.\"\"\"\n",
    "      done: bool\n",
    "\n",
    "# All tools available to the agent\n",
    "tools = [\n",
    "    write_email, \n",
    "    schedule_meeting, \n",
    "    check_calendar_availability, \n",
    "    Question, \n",
    "    Done,\n",
    "]\n",
    "\n",
    "tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "# Initialize the LLM for use with router / structured output\n",
    "llm = init_chat_model(\"openai:gpt-4o\", temperature=0.0)\n",
    "llm_router = llm.with_structured_output(RouterSchema) \n",
    "# Initialize the LLM, enforcing tool use (of any available tools) for agent\n",
    "llm = init_chat_model(\"openai:gpt-4o\", tool_choice=\"required\", temperature=0.0)\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13756006",
   "metadata": {},
   "source": [
    "Now, this is the critical part! Right now we log feedback :\n",
    "```\n",
    "Here is feedback on how the user would prefer the email to be classified\n",
    "```\n",
    "\n",
    "### Memory Management with LangMem\n",
    "\n",
    "But we don't do anything with it! Let's change that by simply adding the feedback to the memory. What we *want* to do is fairly straightforward: we want to add the feedback to the memory `Store`. If we compile our graph with the store, we can access the store in any node. So that is not a problem! But we have to answer two questions: 1) how do we want the memory to be structured? 2) how do we want to update the memory? \n",
    "\n",
    "This is where [LangMem](https://langchain-ai.github.io/langmem/) comes in! LangMem is a lightweight library that can be used on top of the LangGraph Store to provide a more user-friendly API for memory management. We can create a `create_memory_store_manager` that takes care of a few nice things: \n",
    "\n",
    "1) it will create a namespace in the `Store` for us \n",
    "2) it will initialize the store with our default instructions (default_triage_instructions)\n",
    "3) it allows us to specify if we want a collection `enable_inserts=True` of memories\n",
    "4) it allows us to specify if we just want to update one memory \"profile\" `enable_inserts=False`\n",
    "5) it handles updating the memory based upon input messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4b3649",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langmem import create_memory_store_manager\n",
    "from email_assistant.prompts import default_triage_instructions\n",
    "\n",
    "# Feedback memory managers for writing to memory Store \n",
    "triage_feedback_memory_manager = create_memory_store_manager(\n",
    "    init_chat_model(\"openai:gpt-4o\", temperature=0.0),\n",
    "    namespace=(\"email_assistant\", \"triage_preferences\"),\n",
    "    instructions=\"\"\"Extract user email triage preferences into a single set of rules.\n",
    "    Format the information as a string explaining the criteria for each category.\"\"\",\n",
    "    enable_inserts=False, # Update profile in-place,\n",
    "    enable_deletes=False # Do not delete profile from memory\n",
    "    default=default_triage_instructions\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca75b270",
   "metadata": {},
   "source": [
    "Now we can used this in our by directly calling the `invoke` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae1a28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.store.base import BaseStore\n",
    "from langmem import create_memory_store_manager\n",
    "\n",
    "def triage_interrupt_handler(state: State, store: BaseStore) -> Command[Literal[\"response_agent\", \"__end__\"]]:\n",
    "    \"\"\"Handles interrupts from the triage step\"\"\"\n",
    "    \n",
    "    # Parse the email input\n",
    "    author, to, subject, email_thread = parse_email(state[\"email_input\"])\n",
    "\n",
    "    # Create email markdown for Agent Inbox in case of notification  \n",
    "    email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "\n",
    "    # Create messages to save to memory\n",
    "    messages = [{\"role\": \"user\",\n",
    "                \"content\": f\"Classification Decision: {state['classification_decision']} for email: {email_markdown}\"\n",
    "                }]\n",
    "\n",
    "    # Create interrupt for Agent Inbox\n",
    "    request = {\n",
    "        \"action_request\": {\n",
    "            \"action\": f\"Email Assistant: {state['classification_decision']}\",\n",
    "            \"args\": {}\n",
    "        },\n",
    "        \"config\": {\n",
    "            \"allow_ignore\": True,  \n",
    "            \"allow_respond\": True, # Allow user feedback if decision is not correct \n",
    "            \"allow_edit\": False, \n",
    "            \"allow_accept\": False,  \n",
    "        },\n",
    "        # Email to show in Agent Inbox\n",
    "        \"description\": email_markdown,\n",
    "    }\n",
    "\n",
    "    # Agent Inbox responds with a list  \n",
    "    response = interrupt([request])[0]\n",
    "\n",
    "    # Accept the decision and end   \n",
    "    if response[\"type\"] == \"accept\":\n",
    "        goto = END \n",
    "\n",
    "    # If user provides feedback, update memory  \n",
    "    elif response[\"type\"] == \"response\":\n",
    "        # Add feedback to messages \n",
    "        user_input = response[\"args\"]\n",
    "        messages.append({\"role\": \"user\",\n",
    "                        \"content\": f\"Here is feedback on how the user would prefer the email to be classified: {user_input}\"\n",
    "                        })\n",
    "        # Update memory with feedback using the memory manager\n",
    "        triage_feedback_memory_manager.invoke({\"messages\": messages})\n",
    "        goto = END\n",
    "\n",
    "    # Update the state \n",
    "    update = {\n",
    "        \"messages\": messages,\n",
    "    }\n",
    "\n",
    "    return Command(goto=goto, update=update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426ebeaf",
   "metadata": {},
   "source": [
    "We can create a memory manager for each memory type we want to store. \n",
    "\n",
    "Each memory manager is specialized for a specific type of information the assistant needs to remember. Let's examine how we set up different memory types for our email assistant:\n",
    "\n",
    "1. **Response Preferences Manager**: This memory manager captures and maintains user preferences for email responses, such as tone, style, and formatting preferences. It uses a \"profile\" approach (with `enable_inserts=False`) to maintain a single, consolidated set of preferences that get updated over time, rather than creating many discrete memories.\n",
    "\n",
    "2. **Calendar Preferences Manager**: Similar to response preferences, this manager stores scheduling preferences like preferred meeting durations, times of day, and days of the week. It's also implemented as a profile for consistent reference.\n",
    "\n",
    "3. **Background Knowledge Manager**: Unlike the preference managers, this one uses a \"collection\" approach (with `enable_inserts=True`) to accumulate discrete facts about people, projects, and contexts. Each new piece of relevant information becomes a separate memory entry that can be retrieved based on relevance.\n",
    "\n",
    "The key distinction is in how these memories are updated:\n",
    "- Profiles (response and calendar preferences) consolidate feedback into a single document that gets refined over time\n",
    "- Collections (background knowledge) grow by adding new discrete memories while potentially removing outdated ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9523e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "response_preferences_memory_manager = create_memory_store_manager(\n",
    "    llm,\n",
    "    namespace=(\"email_assistant\", \"response_preferences\"),\n",
    "    instructions=\"\"\"You goal is to maintain a profile that contains a user's email response preferences. \n",
    "    If you are given a set of rules, do not remove any rules and simply include them in the resulting profile.\n",
    "    If you are given feedback on an email response, update the profile to reflect the new preferences.\"\"\",\n",
    "    enable_inserts=False, # Update profile in-place,\n",
    "    enable_deletes=False, # Do not delete profile from memory\n",
    "    default=default_response_preferences\n",
    ")\n",
    "\n",
    "cal_preferences_memory_manager = create_memory_store_manager(\n",
    "    llm,\n",
    "    namespace=(\"email_assistant\", \"cal_preferences\"),\n",
    "    instructions=\"\"\"Extract user email calendar preferences into a single set of rules.\n",
    "    Format the information as a string explaining the criteria for each category.\"\"\",\n",
    "    enable_inserts=False, # Update profile in-place,\n",
    "    enable_deletes=False, # Do not delete profile from memory\n",
    "    default=default_cal_preferences\n",
    ")   \n",
    "\n",
    "background_memory_manager = create_memory_store_manager(\n",
    "    llm,\n",
    "    namespace=(\"email_assistant\", \"background\"),\n",
    "    instructions=\"\"\"Extract user email background information about the user, their key connections, and other relevant information.\n",
    "    Format this as a collection of short memories that can be easily recalled.\"\"\",\n",
    "    enable_inserts=True, # Update background in-place,\n",
    "    enable_deletes=True, # Since this is a collection, we can delete items (if they are no longer relevant)\n",
    "    default=default_background\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d0d68",
   "metadata": {},
   "source": [
    "### Accessing Memory in the Triage Router\n",
    "\n",
    "The triage router now leverages stored memory to make more personalized classification decisions. Let's see how memory transforms this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4438162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def triage_router(state: State, store: BaseStore) -> Command[Literal[\"triage_interrupt_handler\", \"response_agent\", \"__end__\"]]:\n",
    "    \"\"\"Analyze email content to decide if we should respond, notify, or ignore.\n",
    "\n",
    "    The triage step prevents the assistant from wasting time on:\n",
    "    - Marketing emails and spam\n",
    "    - Company-wide announcements\n",
    "    - Messages meant for other teams\n",
    "    \"\"\"\n",
    "    \n",
    "    # Search for existing triage_preferences memory\n",
    "    results = triage_feedback_memory_manager.search()\n",
    "    triage_instructions=results[0].value['content']\n",
    "    \n",
    "    # Search for existing background memory\n",
    "    results = background_memory_manager.search()\n",
    "    # Handle collection of memory objects\n",
    "    memories = []\n",
    "    for result in results:\n",
    "        memories.append(result.value['content']['content'])\n",
    "    background_content = \"\\n\".join(memories)\n",
    "        \n",
    "    # Format system prompt with background and triage instructions\n",
    "    system_prompt = triage_system_prompt.format(\n",
    "        background=background_content,\n",
    "        triage_instructions=triage_instructions,\n",
    "    )\n",
    "\n",
    "    # Parse the email input\n",
    "    author, to, subject, email_thread = parse_email(state[\"email_input\"])\n",
    "    user_prompt = triage_user_prompt.format(\n",
    "        author=author, to=to, subject=subject, email_thread=email_thread\n",
    "    )\n",
    "\n",
    "    # Create email markdown for Agent Inbox in case of notification  \n",
    "    email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "\n",
    "    # Run the router LLM\n",
    "    result = llm_router.invoke(\n",
    "        [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Decision\n",
    "    classification = result.classification\n",
    "\n",
    "    # Process the classification decision\n",
    "    if classification == \"respond\":\n",
    "        print(\"📧 Classification: RESPOND - This email requires a response\")\n",
    "        # Next node\n",
    "        goto = \"response_agent\"\n",
    "        # Update the state\n",
    "        update = {\n",
    "            \"classification_decision\": result.classification,\n",
    "            \"messages\": [{\"role\": \"user\",\n",
    "                            \"content\": f\"Respond to the email: {email_markdown}\"\n",
    "                        }],\n",
    "        }\n",
    "    elif classification == \"ignore\":\n",
    "        print(\"🚫 Classification: IGNORE - This email can be safely ignored\")\n",
    "\n",
    "        # Next node\n",
    "        goto = END\n",
    "        # Update the state\n",
    "        update = {\n",
    "            \"classification_decision\": classification,\n",
    "        }\n",
    "\n",
    "    elif classification == \"notify\":\n",
    "        print(\"🔔 Classification: NOTIFY - This email contains important information\") \n",
    "\n",
    "        # Next node\n",
    "        goto = \"triage_interrupt_handler\"\n",
    "        # Update the state\n",
    "        update = {\n",
    "            \"classification_decision\": classification,\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid classification: {classification}\")\n",
    "    return Command(goto=goto, update=update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c514016",
   "metadata": {},
   "source": [
    "### Incorporating Memory into LLM Responses\n",
    "\n",
    "Now that we have memory managers set up, we need to use the stored preferences when generating responses. The `llm_call` function demonstrates how to retrieve and incorporate memory into the LLM's context:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9daeb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_call(state: State, store: BaseStore):\n",
    "    \"\"\"LLM decides whether to call a tool or not\"\"\"\n",
    "\n",
    "    # Search for existing cal_preferences memory\n",
    "    results = cal_preferences_memory_manager.search()\n",
    "    cal_preferences=results[0].value['content']['content']\n",
    "    \n",
    "    # Search for existing response_preferences memory\n",
    "    results = response_preferences_memory_manager.search()\n",
    "    response_preferences=results[0].value['content']['content']\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            llm_with_tools.invoke(\n",
    "                [\n",
    "                    {\"role\": \"system\", \"content\": agent_system_prompt_hitl_memory.format(response_preferences=response_preferences, \n",
    "                                                                                         cal_preferences=cal_preferences)}\n",
    "                ]\n",
    "                + state[\"messages\"]\n",
    "            )\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef187dd9",
   "metadata": {},
   "source": [
    "### Memory Integration in the Interrupt Handler\n",
    "\n",
    "The interrupt handler is where memory truly shines, as it's responsible for capturing user feedback and using it to update our various memory stores. This function showcases how we:\n",
    "\n",
    "1. **Process User Feedback**: When a user edits an email response or provides feedback, we capture that information\n",
    "2. **Update Relevant Memory**: We route the feedback to the appropriate memory manager based on the context\n",
    "3. **Learn Continuously**: Each interaction becomes a learning opportunity for the system\n",
    "\n",
    "Let's break down the key memory interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d599007d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interrupt_handler(state: State, store: BaseStore):\n",
    "    \"\"\"Creates an interrupt for human review of tool calls\"\"\"\n",
    "    \n",
    "    # Store messages\n",
    "    result = []\n",
    "\n",
    "    # Iterate over the tool calls in the last message\n",
    "    for tool_call in state[\"messages\"][-1].tool_calls:\n",
    "        \n",
    "        # Allowed tools for HITL\n",
    "        hitl_tools = [\"write_email\", \"schedule_meeting\", \"Question\"]\n",
    "        \n",
    "        # If tool is not in our HITL list, execute it directly without interruption\n",
    "        if tool_call[\"name\"] not in hitl_tools:\n",
    "            # Execute search_memory and other tools without interruption\n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            observation = tool.invoke(tool_call[\"args\"])\n",
    "            result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": tool_call[\"id\"]})\n",
    "            continue\n",
    "            \n",
    "        # Get original email from email_input in state\n",
    "        original_email_markdown = \"\"\n",
    "        if \"email_input\" in state:\n",
    "            email_input = state[\"email_input\"]\n",
    "            author, to, subject, email_thread = parse_email(email_input)\n",
    "            original_email_markdown = format_email_markdown(subject, author, to, email_thread)\n",
    "        \n",
    "        # Format tool call for display and prepend the original email\n",
    "        tool_display = format_for_display(state, tool_call)\n",
    "        description = original_email_markdown + tool_display\n",
    "\n",
    "        # Configure what actions are allowed in Agent Inbox\n",
    "        if tool_call[\"name\"] == \"write_email\":\n",
    "            config = {\n",
    "                \"allow_ignore\": True,\n",
    "                \"allow_respond\": True,\n",
    "                \"allow_edit\": True,\n",
    "                \"allow_accept\": True,\n",
    "            }\n",
    "        elif tool_call[\"name\"] == \"schedule_meeting\":\n",
    "            config = {\n",
    "                \"allow_ignore\": True,\n",
    "                \"allow_respond\": True,\n",
    "                \"allow_edit\": True,\n",
    "                \"allow_accept\": True,\n",
    "            }\n",
    "        elif tool_call[\"name\"] == \"Question\":\n",
    "            config = {\n",
    "                \"allow_ignore\": True,\n",
    "                \"allow_respond\": True,\n",
    "                \"allow_edit\": False,\n",
    "                \"allow_accept\": False,\n",
    "            }\n",
    "\n",
    "        # Create the interrupt request\n",
    "        request = {\n",
    "            \"action_request\": {\n",
    "                \"action\": tool_call[\"name\"],\n",
    "                \"args\": tool_call[\"args\"]\n",
    "            },\n",
    "            \"config\": config,\n",
    "            \"description\": description,\n",
    "        }\n",
    "\n",
    "        # Send to Agent Inbox and wait for response\n",
    "        response = interrupt([request])[0]\n",
    "\n",
    "        # Handle the responses \n",
    "        if response[\"type\"] == \"accept\":\n",
    "\n",
    "            # Execute the tool with original args\n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            observation = tool.invoke(tool_call[\"args\"])\n",
    "            result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": tool_call[\"id\"]})\n",
    "            \n",
    "            # Remember facts from the conversation with background memory manager\n",
    "            background_memory_manager.invoke({\"messages\": state[\"messages\"] + result})\n",
    "            \n",
    "        elif response[\"type\"] == \"edit\":\n",
    "\n",
    "            # Tool selection \n",
    "            tool = tools_by_name[tool_call[\"name\"]]\n",
    "            \n",
    "            # Get edited args from Agent Inbox\n",
    "            edited_args = response[\"args\"][\"args\"]\n",
    "\n",
    "            # Save feedback in memory and update the write_email tool call with the edited content from Agent Inbox\n",
    "            if tool_call[\"name\"] == \"write_email\":\n",
    "\n",
    "                # We update the memory in the namespace with the messages from the state\n",
    "                response_preferences_memory_manager.invoke({\n",
    "                    \"messages\": state[\"messages\"] + [{\"role\": \"user\", \"content\": f\"Here is a better way to respond to emails: {edited_args}\"}]\n",
    "                })\n",
    "                \n",
    "                # Update the AI message's tool call with edited content (reference to the message in the state)\n",
    "                ai_message = state[\"messages\"][-1]\n",
    "                current_id = tool_call[\"id\"]\n",
    "                \n",
    "                # Replace the original tool call with the edited one (any changes made to this reference affect the original object in the state)\n",
    "                ai_message.tool_calls = [tc for tc in ai_message.tool_calls if tc[\"id\"] != current_id] + [\n",
    "                    {\"type\": \"tool_call\", \"name\": tool_call[\"name\"], \"args\": edited_args, \"id\": current_id}\n",
    "                ]\n",
    "                \n",
    "                # Execute the tool with edited args\n",
    "                observation = tool.invoke(edited_args)\n",
    "                \n",
    "                # Add only the tool response message\n",
    "                result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": current_id})\n",
    "            \n",
    "            # Save feedback in memory and update the schedule_meeting tool call with the edited content from Agent Inbox\n",
    "            elif tool_call[\"name\"] == \"schedule_meeting\":\n",
    "                # Add context about calendar preferences\n",
    "                cal_preferences_memory_manager.invoke({\n",
    "                    \"messages\": state[\"messages\"] + [{\"role\": \"user\", \"content\": f\"Here are preferred calendar settings: {edited_args}\"}]\n",
    "                })\n",
    "                \n",
    "                # Update the AI message's tool call with edited content\n",
    "                ai_message = state[\"messages\"][-1]\n",
    "                current_id = tool_call[\"id\"]\n",
    "                \n",
    "                # Replace the original tool call with the edited one\n",
    "                ai_message.tool_calls = [tc for tc in ai_message.tool_calls if tc[\"id\"] != current_id] + [\n",
    "                    {\"type\": \"tool_call\", \"name\": tool_call[\"name\"], \"args\": edited_args, \"id\": current_id}\n",
    "                ]\n",
    "                \n",
    "                # Execute the tool with edited args\n",
    "                observation = tool.invoke(edited_args)\n",
    "                \n",
    "                # Add only the tool response message\n",
    "                result.append({\"role\": \"tool\", \"content\": observation, \"tool_call_id\": current_id})\n",
    "\n",
    "        elif response[\"type\"] == \"ignore\":\n",
    "            # Update relevant domain-specific memory\n",
    "            if tool_call[\"name\"] == \"write_email\":\n",
    "                # Add context about email response preferences\n",
    "                response_preferences_memory_manager.invoke({\n",
    "                    \"messages\": state[\"messages\"] + [{\"role\": \"user\", \"content\": f\"User decided to ignore this email! Make note of this as an few shot example.\"}]\n",
    "                })\n",
    "            elif tool_call[\"name\"] == \"schedule_meeting\":\n",
    "                # Add context about calendar preferences\n",
    "                cal_preferences_memory_manager.invoke({\n",
    "                    \"messages\": state[\"messages\"] + [{\"role\": \"user\", \"content\": f\"User decided to ignore this email! Make note of this as an few shot example.\"}]\n",
    "                })\n",
    "\n",
    "        elif response[\"type\"] == \"response\":\n",
    "            # User provided feedback\n",
    "            user_feedback = response[\"args\"]\n",
    "            result.append({\"role\": \"tool\", \"content\": f\"Feedback: {user_feedback}\", \"tool_call_id\": tool_call[\"id\"]})\n",
    "            # Also update relevant domain-specific memory\n",
    "            if tool_call[\"name\"] == \"write_email\":\n",
    "                # Add context about email response preferences\n",
    "                response_preferences_memory_manager.invoke({\n",
    "                    \"messages\": state[\"messages\"] + [{\"role\": \"user\", \"content\": f\"Here is feedback on how to respond to emails: {user_feedback}\"}]\n",
    "                })\n",
    "            elif tool_call[\"name\"] == \"schedule_meeting\":\n",
    "                # Add context about calendar preferences\n",
    "                cal_preferences_memory_manager.invoke({\n",
    "                    \"messages\": state[\"messages\"] + [{\"role\": \"user\", \"content\": f\"Here is feedback on calendar scheduling: {user_feedback}\"}]\n",
    "                })\n",
    "\n",
    "    return {\"messages\": result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b6f30e",
   "metadata": {},
   "source": [
    "This is the same as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ffd2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Conditional edge function\n",
    "def should_continue(state: State) -> Literal[\"interrupt_handler\", END]:\n",
    "    \"\"\"Route to tool handler, or end if Done tool called\"\"\"\n",
    "    messages = state[\"messages\"]\n",
    "    last_message = messages[-1]\n",
    "    if last_message.tool_calls:\n",
    "        for tool_call in last_message.tool_calls: \n",
    "            if tool_call[\"name\"] == \"Done\":\n",
    "                return END\n",
    "            else:\n",
    "                return \"interrupt_handler\"\n",
    "\n",
    "# Build workflow\n",
    "agent_builder = StateGraph(State)\n",
    "\n",
    "# Add nodes - with store parameter\n",
    "agent_builder.add_node(\"llm_call\", llm_call)\n",
    "agent_builder.add_node(\"interrupt_handler\", interrupt_handler)\n",
    "\n",
    "# Add edges\n",
    "agent_builder.add_edge(START, \"llm_call\")\n",
    "agent_builder.add_conditional_edges(\n",
    "    \"llm_call\",\n",
    "    should_continue,\n",
    "    {\n",
    "        \"interrupt_handler\": \"interrupt_handler\",\n",
    "        END: END,\n",
    "    },\n",
    ")\n",
    "agent_builder.add_edge(\"interrupt_handler\", \"llm_call\")\n",
    "\n",
    "# Compile the agent - nodes will receive store parameter automatically\n",
    "response_agent = agent_builder.compile()\n",
    "\n",
    "# Build overall workflow with store and checkpointer\n",
    "overall_workflow = (\n",
    "    StateGraph(State, input=StateInput)\n",
    "    .add_node(triage_router)\n",
    "    .add_node(triage_interrupt_handler)\n",
    "    .add_node(\"response_agent\", response_agent)\n",
    "    .add_edge(START, \"triage_router\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6fca2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Now, we can compile the graph with the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff24d1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid \n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from src.email_assistant.email_assistant_hitl_memory import overall_workflow\n",
    "checkpointer = MemorySaver()\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Respond\n",
    "email_input =  {\n",
    "    \"to\": \"Lance Martin <lance@company.com>\",\n",
    "    \"author\": \"Project Manager <pm@client.com>\",\n",
    "    \"subject\": \"Tax season let's schedule call\",\n",
    "    \"email_thread\": \"Lance,\\n\\nIt's tax season again, and I wanted to schedule a call to discuss your tax planning strategies for this year. I have some suggestions that could potentially save you money.\\n\\nAre you available sometime next week? Tuesday or Thursday afternoon would work best for me, for about 45 minutes.\\n\\nRegards,\\nProject Manager\"\n",
    "}\n",
    "\n",
    "# Compile the graph\n",
    "graph = overall_workflow.compile(checkpointer=checkpointer, store=store)\n",
    "thread_config = {\"configurable\": {\"thread_id\": uuid.uuid4()}}\n",
    "\n",
    "# Run the graph until the first interrupt\n",
    "for chunk in graph.stream({\"email_input\": email_input}, config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c96edc8a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "We can see the initial preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f273cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the initial response preferences\n",
    "results = store.search((\"email_assistant\", \"triage_preferences\"))\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06500db5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "And:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aceba438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the initial response preferences\n",
    "results = store.search((\"email_assistant\", \"cal_preferences\"))\n",
    "results[0].value['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700fbe4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Add feedback:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18395d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.types import Command\n",
    "# response = adds FEEDBACK for future reference, which is not use yet! We need memory to use it.\n",
    "for chunk in graph.stream(Command(resume=[{\"type\": \"response\", \n",
    "                                          \"args\": \"Always use 30 minute calls in the future!'\"}]), config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a76af8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Updated preferences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d960cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can see the updated response preferences\n",
    "results = store.search((\"email_assistant\", \"cal_preferences\"))\n",
    "results[0].value['content']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f51d10b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Then, accept the calendar invite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1345370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accept the invite\n",
    "for chunk in graph.stream(Command(resume=[{\"type\": \"accept\", \n",
    "                                          \"args\": \"\"}]), config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81fad26",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "And accept the email to send:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9dd514",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Accept the email to send\n",
    "for chunk in graph.stream(Command(resume=[{\"type\": \"accept\", \n",
    "                                          \"args\": \"\"}]), config=thread_config):\n",
    "   print(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e434689",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
