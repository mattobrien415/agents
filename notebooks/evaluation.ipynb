{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68e21aa1",
   "metadata": {},
   "source": [
    "# Evaluating Agents\n",
    "\n",
    "We have two implementations of our email assistant. One is an agent, and one is an agentic workflow that uses a router to triage the email and then passes the email to the agent for response generation. How can we compare them? As an example, we may want to know how they compare in terms of token usage, response quality, latency, or triage accuracy. This is why testing is important: it guides your decisions about architecture. [LangSmith](https://docs.smith.langchain.com/) offers two primary ways to test agents. \n",
    "\n",
    "![overview-img](img/overview_eval.png)\n",
    "\n",
    "## Test Approaches \n",
    "\n",
    "### Pytest\n",
    "\n",
    "[Pytest](https://docs.pytest.org/en/stable/) is well known to many developers as a powerful tool for writing tests within the Python ecosystem. LangSmith integrates with pytest to allow you to write tests that we can run on each assistant and log the results to LangSmith.\n",
    "\n",
    "### LangSmith Datasets \n",
    "\n",
    "You can also create a dataset [in LangSmith](https://docs.smith.langchain.com/evaluation) and run each assistant against the dataset using the LangSmith evaluate API. TODO: Add pro/con comparing Pytest to LangSmith datasets.  \n",
    "\n",
    "## Test Cases\n",
    "\n",
    "Testing often starts with defining the test cases, which can be a challenging process. In this case, we'll just define a set of example emails we want to handle along with a few things to test. You can see the test cases in `eval/email_dataset.py`, which contains the following:\n",
    "\n",
    "1. **Input Emails**: A collection of diverse email examples\n",
    "2. **Ground Truth Classifications**: `Respond`, `Notify`, `Ignore`\n",
    "3. **Expected Tool Calls**: Tools called for each email that requires a response\n",
    "4. **Response Criteria**: What makes a good response for emails requiring replies\n",
    "\n",
    "## Pytest Example\n",
    "\n",
    "Here's a simple example of testing using Pytest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d98511",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f158f4",
   "metadata": {},
   "source": [
    "We will test whether the `baseline_agent` makes the appropriate tool calls when responding to the emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae92fe30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from eval.email_dataset import email_inputs, expected_tool_calls\n",
    "from email_assistant.utils import format_messages_string\n",
    "from email_assistant.baseline_agent import overall_workflow\n",
    "from email_assistant.utils import extract_tool_calls\n",
    "\n",
    "from langsmith import testing as t\n",
    "\n",
    "@pytest.mark.langsmith\n",
    "@pytest.mark.parametrize(\n",
    "    \"email_input, expected_calls\",\n",
    "    [   # Pick some examples with e-mail reply expected\n",
    "        (email_inputs[0],expected_tool_calls[0]),\n",
    "        (email_inputs[3],expected_tool_calls[3]),\n",
    "    ],\n",
    ")\n",
    "def test_email_dataset_tool_calls(email_input, expected_calls):\n",
    "    \"\"\"Test if email processing contains expected tool calls.\"\"\"\n",
    "    \n",
    "    # Set up the assistant\n",
    "    email_assistant = overall_workflow.compile()\n",
    "    \n",
    "    # Run the baseline agent\n",
    "    messages = [{\"role\": \"user\", \"content\": str(email_input)}]\n",
    "    result = email_assistant.invoke({\"messages\": messages})\n",
    "            \n",
    "    # Extract tool calls from messages list\n",
    "    extracted_tool_calls = extract_tool_calls(result['messages'])\n",
    "            \n",
    "    # Check if all expected tool calls are in the extracted ones\n",
    "    missing_calls = [call for call in expected_calls if call.lower() not in extracted_tool_calls]\n",
    "    \n",
    "    t.log_outputs({\n",
    "                \"missing_calls\": missing_calls,\n",
    "                \"extracted_tool_calls\": extracted_tool_calls,\n",
    "                \"response\": format_messages_string(result['messages'])\n",
    "            })\n",
    "\n",
    "    # Test passes if no expected calls are missing\n",
    "    assert len(missing_calls) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "700aba2a",
   "metadata": {},
   "source": [
    "You'll notice a few things. First, to [run with Pytest and log test results to LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest), we only need to add the `@pytest.mark.langsmith ` decorator to our function and place it in a file, as you see in `notebooks/test_tools.py`. Second, we can pass dataset examples to the test function as shown [here](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parametrize-with-pytestmarkparametrize) via `@pytest.mark.parametrize`. We can run the test from the command line. From the project root, run:\n",
    "\n",
    "```\n",
    "! LANGSMITH_TEST_SUITE='Email assistant: Test Tools'  pytest notebooks/test_tools.py\n",
    "```\n",
    "\n",
    "We can view the results in the LangSmith UI. The `assert len(missing_calls) == 0` is logged to the `Pass` column in LangSmith. The `log_outputs` are passed to the `Outputs` column and function arguments are passed to the `Inputs` column. Each input passed in `@pytest.mark.parametrize(` is a separate row logged to the `LANGSMITH_TEST_SUITE` project name in LangSmith, which is found under `Datasets & Experiments`.\n",
    "\n",
    "![Test Results](img/test_result.png)\n",
    "\n",
    "## LangSmith Datasets \n",
    "\n",
    "### Dataset Definition \n",
    "\n",
    "In addition to the Pytest approach, we can also [create a dataset in LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset) with the LangSmith SDK. This creates a dataset with the test cases in the `eval/email_dataset.py` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea997ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from eval.email_dataset import examples_triage\n",
    "from email_assistant.email_assistant import email_assistant\n",
    "from email_assistant.baseline_agent import email_assistant as baseline_agent\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"Interrupt Workshop: E-mail Triage Dataset\"\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, \n",
    "        description=\"A dataset of e-mails and their triage decisions.\"\n",
    "    )\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples_triage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2df606",
   "metadata": {},
   "source": [
    "### Run Agents \n",
    "\n",
    "The dataset has the following structure, with an e-mail input and a ground truth classification for the e-mail as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3891c122",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_triage = [\n",
    "  {\n",
    "      \"inputs\": {\"email_input\": email_input_1},\n",
    "      \"outputs\": {\"classification\": triage_output_1},\n",
    "  }, ...\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8290e820",
   "metadata": {},
   "source": [
    "We define functions that take dataset inputs and pass them to each agent we want to evaluate. The function just takes the `inputs` dict from the dataset and passes it to the agent. It returns a dict with the agent's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9d1ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_email_assistant(inputs: dict) -> dict:\n",
    "    \"\"\"Process an email through the workflow-based email assistant.\"\"\"\n",
    "    response = email_assistant.invoke({\"email_input\": inputs[\"email_input\"]})\n",
    "    return {\"classification_decision\": response['classification_decision']}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba6ec4c",
   "metadata": {},
   "source": [
    "The LangSmith [evaluate API](https://docs.smith.langchain.com/evaluation) passes the `inputs` dict to this function. \n",
    "\n",
    "### Evaluator Function \n",
    "\n",
    "We also create an evaluator function. What do we want to evaluate? We have reference outputs in our dataset and agent outputs defined in the functions above.\n",
    "\n",
    "* Reference outputs: `\"outputs\": {\"classification\": triage_output_1} ...`\n",
    "* Agent outputs: `\"outputs\": {\"classification_decision\": agent_output_1} ...`\n",
    "\n",
    "We want to evaluate if the agent's output matches the reference output. So we simply need a an evaluator function that compares the two, where `outputs` is the agent's output and `reference_outputs` is the reference output from the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fee7532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_evaluator(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fd2de9",
   "metadata": {},
   "source": [
    "### Running Evaluation\n",
    "\n",
    "Now, the question is: how are these things hooked together? The evaluate API takes care of it for us. It passes the `inputs` dict from our dataset the target function. It passes the `outputs` dict from our dataset to the evaluator function. And it passes the output of our agent to the evaluator function. Note this is similar to what we did with Pytest: in Pytest, we passed in the dataset example inputs and reference outputs to the test function with `@pytest.mark.parametrize`.\n",
    "\n",
    "![overview-img](img/eval_detail.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6807306d",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_workflow = client.evaluate(\n",
    "    # Run agent \n",
    "    target_email_assistant,\n",
    "    # Dataset name   \n",
    "    data=dataset_name,\n",
    "    # Evaluator\n",
    "    evaluators=[classification_evaluator],\n",
    "    # Name of the experiment\n",
    "    experiment_prefix=\"E-mail assistant workflow\", \n",
    "    # Number of concurrent evaluations\n",
    "    max_concurrency=2, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76baff88",
   "metadata": {},
   "source": [
    "We can view the results from both experiments in the LangSmith UI.\n",
    "\n",
    "![Test Results](img/eval.png)\n",
    "\n",
    "### Getting Results\n",
    "\n",
    "We can also get the results of the evaluation, which are returned as `EvaluationResult` objects and can be converted to pandas dataframes. This is great if we want to create our own visualizations to compare the performance of the two agents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b655f8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Convert evaluation results to pandas dataframes\n",
    "df_workflow = experiment_results_workflow.to_pandas()\n",
    "\n",
    "# Calculate mean scores (values are on a 0-1 scale)\n",
    "workflow_score = df_workflow[f'feedback.classification_evaluator'].mean()\n",
    "\n",
    "# Create a bar plot comparing the two models\n",
    "plt.figure(figsize=(10, 6))\n",
    "models = ['Tool Calling Agent', 'Agentic Workflow']\n",
    "scores = [workflow_score]\n",
    "\n",
    "# Create and save a bar chart\n",
    "plt.bar(models, scores, color=['#5DA5DA', '#FAA43A'], width=0.5)\n",
    "plt.xlabel('Agent Type')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Email Triage Performance Comparison - Classification Score')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef1220b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
