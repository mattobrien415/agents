{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04c6700",
   "metadata": {},
   "source": [
    "# Evaluating Agents\n",
    "\n",
    "We have two implementations of our email assistant:\n",
    "\n",
    "* `src/email_assistant/baseline_agent.py` \n",
    "* `src/email_assistant/email_assistant.py`\n",
    "\n",
    "How can we compare them? [LangSmith](https://docs.smith.langchain.com/) offers two primary ways to test agents. \n",
    "\n",
    "## Test Approaches \n",
    "\n",
    "### Pytest\n",
    "\n",
    "[Pytest](https://docs.pytest.org/en/stable/) is well known to many developers. It is a powerful tool for writing tests and is well integrated with the Python ecosystem. LangSmith integrates with pytest to allow you to write tests that we can run on each assistant.\n",
    "\n",
    "### LangSmith Datasets \n",
    "\n",
    "You can also create a dataset [in LangSmith](https://docs.smith.langchain.com/evaluation) and run each assistant against the dataset using the LangSmith evaluate API. This has some useful features, such as the ability to directly use the results from custom visualizations, as we'll see. \n",
    "\n",
    "## Test Cases\n",
    "\n",
    "Both evaluation approaches use a common structure for test data:\n",
    "\n",
    "1. **Input Emails**: A collection of diverse email examples\n",
    "2. **Ground Truth Classifications**: Respond, notify, ignore\n",
    "3. **Expected Tool Calls**: Tools called for each email that requires a response\n",
    "4. **Response Criteria**: What makes a good response for emails requiring replies\n",
    "\n",
    "## Pytest Example\n",
    "\n",
    "Here's a simple example using Pytest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc59044b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd ..\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e68b6",
   "metadata": {},
   "source": [
    "We will test whether the `baseline_agent` makes the appropriate tool calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c87e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "from eval.email_dataset import email_inputs, expected_tool_calls\n",
    "from email_assistant.utils import format_messages_string\n",
    "from email_assistant.baseline_agent import overall_workflow\n",
    "from email_assistant.utils import extract_tool_calls\n",
    "\n",
    "from langsmith import testing as t\n",
    "\n",
    "@pytest.mark.langsmith\n",
    "@pytest.mark.parametrize(\n",
    "    \"email_input, expected_calls\",\n",
    "    [   # Pick some examples with e-mail reply expected\n",
    "        (email_inputs[0],expected_tool_calls[0]),\n",
    "        (email_inputs[3],expected_tool_calls[3]),\n",
    "    ],\n",
    ")\n",
    "def test_email_dataset_tool_calls(email_input, expected_calls):\n",
    "    \"\"\"Test if email processing contains expected tool calls.\"\"\"\n",
    "    \n",
    "    # Set up the assistant\n",
    "    email_assistant = overall_workflow.compile()\n",
    "    \n",
    "    # Run the baseline agent\n",
    "    messages = [{\"role\": \"user\", \"content\": str(email_input)}]\n",
    "    result = email_assistant.invoke({\"messages\": messages})\n",
    "            \n",
    "    # Extract tool calls from messages list\n",
    "    extracted_tool_calls = extract_tool_calls(result['messages'])\n",
    "            \n",
    "    # Check if all expected tool calls are in the extracted ones\n",
    "    missing_calls = [call for call in expected_calls if call.lower() not in extracted_tool_calls]\n",
    "    \n",
    "    t.log_outputs({\n",
    "                \"missing_calls\": missing_calls,\n",
    "                \"extracted_tool_calls\": extracted_tool_calls,\n",
    "                \"response\": format_messages_string(result['messages'])\n",
    "            })\n",
    "\n",
    "    # Test passes if no expected calls are missing\n",
    "    assert len(missing_calls) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00db74d6",
   "metadata": {},
   "source": [
    "To [run with Pytest and log test results to LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest), we only need to add the `@pytest.mark.langsmith ` decorator to our function and place it in a file, as you see in `test_tools.py`. \n",
    "\n",
    "Note that we pass examples to the function shown [here](https://docs.smith.langchain.com/evaluation/how_to_guides/pytest#parametrize-with-pytestmarkparametrize) via `@pytest.mark.parametrize`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053508a1",
   "metadata": {},
   "source": [
    "We can run the test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad638a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "$ LANGSMITH_TEST_SUITE='Email assistant: Test Tools'  pytest notebooks/test_tools.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04ff655",
   "metadata": {},
   "source": [
    "We can view the results in the LangSmith UI. The `assert len(missing_calls) == 0` is logged to the `Pass` column in LangSmith. The `log_outputs` are passed to the Outputs column and function arguments are passed to the Inputs column.\n",
    "\n",
    "![Test Results](img/test_result.png)\n",
    "\n",
    "## LangSmith Datasets \n",
    "\n",
    "### Dataset Definition \n",
    "\n",
    "We can also [create a dataset in LangSmith](https://docs.smith.langchain.com/evaluation/how_to_guides/manage_datasets_programmatically#create-a-dataset) with the SDK. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99a8a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from eval.email_dataset import examples_triage\n",
    "from email_assistant.email_assistant import email_assistant\n",
    "from email_assistant.baseline_agent import email_assistant as baseline_agent\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "# Dataset name\n",
    "dataset_name = \"Interrupt Workshop: E-mail Triage Dataset\"\n",
    "\n",
    "# Create dataset if it doesn't exist\n",
    "if not client.has_dataset(dataset_name=dataset_name):\n",
    "    dataset = client.create_dataset(\n",
    "        dataset_name=dataset_name, \n",
    "        description=\"A dataset of e-mails and their triage decisions.\"\n",
    "    )\n",
    "    # Add examples to the dataset\n",
    "    client.create_examples(dataset_id=dataset.id, examples=examples_triage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ead203",
   "metadata": {},
   "source": [
    "### Run Agents \n",
    "\n",
    "The dataset pulls from the `eval/email_dataset.py` file.\n",
    "\n",
    "It has the following structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49505bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "examples_triage = [\n",
    "  {\n",
    "      \"inputs\": {\"email_input\": email_input_1},\n",
    "      \"outputs\": {\"classification\": triage_output_1},\n",
    "  },"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07da02ee",
   "metadata": {},
   "source": [
    "We define functions that take dataset inputs and pass them to each agent: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fcd392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_email_assistant(inputs: dict) -> dict:\n",
    "    \"\"\"Process an email through the workflow-based email assistant.\"\"\"\n",
    "    try:\n",
    "        response = email_assistant.invoke({\"email_input\": inputs[\"email_input\"]})\n",
    "        if \"classification_decision\" in response:\n",
    "            return {\"classification_decision\": response['classification_decision']}\n",
    "        else:\n",
    "            return {\"classification_decision\": \"unknown\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in workflow agent: {e}\")\n",
    "        return {\"classification_decision\": \"unknown\"}\n",
    "\n",
    "def target_email_assistant_baseline(inputs: dict) -> dict:\n",
    "    \"\"\"Process an email through the baseline email assistant.\"\"\"\n",
    "    try:\n",
    "        # Format a better prompt for the baseline agent\n",
    "        email_content = inputs[\"email_input\"]\n",
    "        formatted_content = f\"\"\"\n",
    "From: {email_content.get('author', 'Unknown')}\n",
    "To: {email_content.get('to', 'Unknown')}\n",
    "Subject: {email_content.get('subject', 'No Subject')}\n",
    "\n",
    "{email_content.get('email_thread', '')}\n",
    "\"\"\"\n",
    "        messages = [{\"role\": \"user\", \"content\": f\"Please triage this email: {formatted_content}\"}]\n",
    "        \n",
    "        response = baseline_agent.invoke({\"messages\": messages})\n",
    "        if \"classification_decision\" in response:\n",
    "            return {\"classification_decision\": response['classification_decision']}\n",
    "        else:\n",
    "            return {\"classification_decision\": \"unknown\"}\n",
    "    except Exception as e:\n",
    "        print(f\"Error in baseline agent: {e}\")\n",
    "        return {\"classification_decision\": \"unknown\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0b9ee1",
   "metadata": {},
   "source": [
    "The LangSmith evaluate API passes the `inputs` dict to this function. \n",
    "\n",
    "### Evaluator Function \n",
    "\n",
    "We also create an evaluator function. The LangSmith evaluate API passes the `output` dict from our dataset (`reference_outputs[\"classification\"]`)to this function. It also passes the output of the agent (`outputs[\"classification_decision\"]`) to be evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d780f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_evaluator(outputs: dict, reference_outputs: dict) -> bool:\n",
    "    \"\"\"Check if the answer exactly matches the expected answer.\"\"\"\n",
    "    return outputs[\"classification_decision\"].lower() == reference_outputs[\"classification\"].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fe1857",
   "metadata": {},
   "source": [
    "### Running Evaluation\n",
    "\n",
    "We run evaluations for both the baseline agent and the workflow-based agent using the LangSmith evaluate API, which passes the `inputs` dict from our dataset the target function and the `outputs` dict from our dataset to the evaluator function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d10914",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_results_baseline = client.evaluate(\n",
    "    # Run agent  \n",
    "    target_email_assistant_baseline,\n",
    "    # Dataset name  \n",
    "    data=dataset_name,\n",
    "    # Evaluator\n",
    "    evaluators=[classification_evaluator],\n",
    "    # Name of the experiment\n",
    "    experiment_prefix=\"E-mail assistant baseline\", \n",
    "    # Number of concurrent evaluations\n",
    "    max_concurrency=2, \n",
    ")\n",
    "\n",
    "experiment_results_workflow = client.evaluate(\n",
    "    # Run agent \n",
    "    target_email_assistant,\n",
    "    # Dataset name   \n",
    "    data=dataset_name,\n",
    "    # Evaluator\n",
    "    evaluators=[classification_evaluator],\n",
    "    # Name of the experiment\n",
    "    experiment_prefix=\"E-mail assistant workflow\", \n",
    "    # Number of concurrent evaluations\n",
    "    max_concurrency=2, \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b205457",
   "metadata": {},
   "source": [
    "We can view the results in the LangSmith UI.\n",
    "\n",
    "![Test Results](img/eval.png)\n",
    "\n",
    "### Getting Results\n",
    "\n",
    "We can easily get the results of the evaluation. This is great if we want to create a visualization to compare the performance of the two agent types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdeba8a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Convert evaluation results to pandas dataframes\n",
    "df_baseline = experiment_results_baseline.to_pandas()\n",
    "df_workflow = experiment_results_workflow.to_pandas()\n",
    "\n",
    "# Calculate mean scores (values are on a 0-1 scale)\n",
    "baseline_score = df_baseline[f'feedback.classification_evaluator'].mean()\n",
    "workflow_score = df_workflow[f'feedback.classification_evaluator'].mean()\n",
    "\n",
    "# Create a bar plot comparing the two models\n",
    "plt.figure(figsize=(10, 6))\n",
    "models = ['Tool Calling Agent', 'Agentic Workflow']\n",
    "scores = [baseline_score, workflow_score]\n",
    "\n",
    "# Create and save a bar chart\n",
    "plt.bar(models, scores, color=['#5DA5DA', '#FAA43A'], width=0.5)\n",
    "plt.xlabel('Agent Type')\n",
    "plt.ylabel('Average Score')\n",
    "plt.title('Email Triage Performance Comparison - Classification Score')\n",
    "plt.ylim(0, 1.1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb8f0fa",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
